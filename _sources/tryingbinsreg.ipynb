{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual EDA\n",
    "\n",
    "The [first page of this chapter](04b-whyplot) discussed the reasons we plot our data. \n",
    "1. Data cleaning: To find issues in the data that need to get fixed before we can do larger analysis\n",
    "2. Data exploration: Learning about each of the variables, how they covary, and what further questions you can ask of the data\n",
    "3. Analysis and presentation\n",
    "\n",
    "## EDA on a classic firm financial dataset\n",
    "\n",
    "In [the Pandas EDA](02e_eda_golden) page, I explored Compustat by producing summary stats to get a sense of the variables involved, look for missing values, and look for problematic outliers. We noted that some variables, like $delaycon$, had a lot of missing values and decided we'd look into it. \n",
    "\n",
    "Let's continue exploring that dataset. First, let's download our slice of it. The variables are listed and described in a csv file in the [repo's data folder.](https://github.com/LeDataSciFi/ledatascifi-2021/tree/main/data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://seaborn.pydata.org/generated/seaborn.PairGrid.html#seaborn.PairGrid\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.PairGrid.map.html#seaborn.PairGrid.map\n",
    "\n",
    "https://seaborn.pydata.org/generated/seaborn.scatterplot.html#seaborn.scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'binsreg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DONSLA~1\\AppData\\Local\\Temp/ipykernel_28416/2704701655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbinsreg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbinsreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'binsreg'"
     ]
    }
   ],
   "source": [
    "from binsreg import binsreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# these three are used to download the file\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = 'https://github.com/LeDataSciFi/ledatascifi-2021/blob/main/data/CCM_cleaned_for_class.zip?raw=true'\n",
    "\n",
    "#firms = pd.read_stata(url)   \n",
    "# <-- that code would work, but GH said it was too big and\n",
    "# forced me to zip it, so here is the work around to download it:\n",
    "\n",
    "with urlopen(url) as request:\n",
    "    data = BytesIO(request.read())\n",
    "\n",
    "with ZipFile(data) as archive:\n",
    "    with archive.open(archive.namelist()[0]) as stata:\n",
    "        ccm = pd.read_stata(stata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(ccm[['capx_a', 'xrd_a', 'cash_a','td_a']])\n",
    "g.map_lower(binsreg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f1 = sns.pairplot(ccm[['capx_a', 'xrd_a', 'cash_a','td_a']].sample(500),\n",
    "                kind='reg',\n",
    "                 corner=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariances and relationships between variables\n",
    "\n",
    "To get a quick sense of relationships, I like to use I like to use `pairplot` and `heatmap` to get a quick since of relationships.\n",
    "\n",
    "### Getting the big picture with [**Pairplot**](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
    "\n",
    "I like passing `corner=True` or using the `x_vars` and `y_vars` parameters to make the info shown more usable. \n",
    "\n",
    "```{warning}\n",
    "With pairplot, \n",
    "1. Use 7 or fewer variables at a time. If your dataset has a lot of variables, do them part by part.\n",
    "2. Don't plot all of the data points! This will oversaturate your graphs and make it harder to draw any conclusions. Below, I randomly sample a piece of the dataset. \n",
    "```\n",
    "\n",
    "**It's clear from running these two plots that some extreme outliers are hiding patterns by messing with the scales and influencing the regression lines.**\n",
    "\n",
    "(We should deal with these outliers later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every time you run this, you'll get diff figures... why?!\n",
    "f1 = sns.pairplot(ccm[['capx_a', 'xrd_a', 'cash_a','td_a']].sample(500),\n",
    "                kind='reg',\n",
    "                 corner=True)\n",
    "f2 = sns.pairplot(ccm[['capx_a', 'xrd_a', 'cash_a','td_a']].sample(500),\n",
    "                kind='hist',\n",
    "                 corner=True) # hist handles a lot of datapoints well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the big picture with [**Heatmap with correlations**](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
    "\n",
    "After some pairplots (and often before), I like to look at correlations.\n",
    "\n",
    "```{warning}\n",
    "This analysis step doesn't help for categorical variables!\n",
    "\n",
    "Make sure you don't include categorical variables that are numbers!\n",
    "\n",
    "(E.g. industry classifications are numbers that have no meaning.)\n",
    "```\n",
    "\n",
    "Seeing the correlations between variables is nice.\n",
    "\n",
    "A correlation table is ugly and hard to work with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccm.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a lazily made figure of that exact same info is somewhat workable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = sns.heatmap(ccm.corr()) # v1, use the nicer version below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning that and making it more useful is easy:\n",
    "1. Drop the numerical variables that don't make sense in a correlation matrix\n",
    "2. Make the figure large enough to see\n",
    "3. Colors: cold for negative corr, hot for positive corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont plot identifying type info or categorical vars\n",
    "corr = ccm.drop(columns=['gvkey','lpermno','sic3','fyear','sic']).corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,9)) # make a big space for the figure\n",
    "ax = sns.heatmap(corr,\n",
    "                 # cmap for the colors, \n",
    "                 center=0,square=True,\n",
    "                 cmap=sns.diverging_palette(230, 20, as_cmap=True),\n",
    "                 # mask to hide the upper diag (redundant)\n",
    "                 mask=np.triu(np.ones_like(corr, dtype=bool)),\n",
    "                 # shrink the heat legend\n",
    "                 cbar_kws={\"shrink\": .5},\n",
    "                 #optional: vmax and vmin will \"cap\" the color range\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is an information DENSE figure, but we somehow managed to get it on screen decently! Still, it's a ton of variables, and doing this in parts would be a good idea.\n",
    "\n",
    "```{tip}\n",
    "If you're feeling frisky, and your data is in good shape, you can push this farther by using [`sns.clustermap`](https://seaborn.pydata.org/generated/seaborn.clustermap.html) to find clusters of similar variables. \n",
    "```\n",
    "\n",
    "Also - don't take these correlations as gospel yet: They should *point* you towards further relationships to explore, which you should do one plot at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digging in with [**lmplot**](https://seaborn.pydata.org/generated/seaborn.relplot.html) and [**Jointplot**](https://seaborn.pydata.org/generated/seaborn.jointplot.html)\n",
    "\n",
    "These are good for digging into the relationships between two continuous variables. \n",
    "\n",
    "Let's dig into a strong correlation suggested by our heatmap.\n",
    "\n",
    "```{warning}\n",
    "\n",
    "Jointplot can be slow - it's doing a lot. \n",
    "\n",
    "Again, don't plot all of the data points! As your sample size goes up, either randomly sample data, or use \"hex\" style graphs. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = sns.jointplot(data=ccm.query('xrd_a<.4').sample(1000),\n",
    "                  x=\"prodmktfluid\", y=\"xrd_a\", kind='reg') \n",
    "# notice: most firms have 0 R&D!\n",
    "f2 = sns.jointplot(data=ccm.query('xrd_a<.4 & xrd_a > 0').sample(1000),\n",
    "                  x=\"prodmktfluid\", y=\"xrd_a\", kind='reg')\n",
    "\n",
    "# set_title doesn't work with jointplots\n",
    "f1.fig.suptitle('Strongly positive, even with zero R&D firms in sample')\n",
    "f1.fig.subplots_adjust(top=0.95) # Reduce plot to make room \n",
    "f2.fig.suptitle('Among R&D firms, even stronger relationship')\n",
    "f2.fig.subplots_adjust(top=0.95) # Reduce plot to make room \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd pencil this as a relationship to look into more (Do firms do more R&D _**because**_ of the fluidity of their product market?) and then continue exploring. \n",
    "\n",
    "`lmplot` will plot regressions as well, but it makes it easy add [facets](04d-whichplot) to see if the relationship depends on a third (categorical) variable with the `hue`, `col`, and `row` parameters. (And you can combine `hue`, `col`, and `row` to see several cuts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3 = sns.lmplot(data=ccm.query('xrd_a<.4 & xrd_a > 0').sample(1000),\n",
    "                  x=\"prodmktfluid\", y=\"xrd_a\", hue='div_d')\n",
    "f4 = sns.lmplot(data=ccm.query('xrd_a<.4 & xrd_a > 0').sample(1000),\n",
    "                  x=\"prodmktfluid\", y=\"xrd_a\", col='div_d')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
